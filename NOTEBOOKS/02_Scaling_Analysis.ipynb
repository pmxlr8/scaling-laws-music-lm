{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad87dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 2: SETUP + DRIVE STAGING ===\n",
    "from google.colab import drive\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîß Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# === PATHS (single source of truth) ===\n",
    "DRIVE_BASE = '/content/drive/MyDrive/NYU_ML_Project'\n",
    "DRIVE_TARBALL = f'{DRIVE_BASE}/Data/lmd_full.tar.gz'\n",
    "\n",
    "PROCESSED_DIR = f'{DRIVE_BASE}/Data/processed_v3'\n",
    "CHECKPOINT_DIR = f'{DRIVE_BASE}/checkpoints_v3'\n",
    "OUTPUT_DIR = f'{DRIVE_BASE}/outputs'\n",
    "\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === STAGE DATA TO LOCAL DISK ===\n",
    "LOCAL_ABC_DIR = '/content/abc_corpus'\n",
    "\n",
    "def find_abc_files(root, max_count=5000, timeout=10):\n",
    "    \"\"\"Quick check if directory has .abc files.\"\"\"\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for fn in filenames:\n",
    "            if fn.endswith('.abc'):\n",
    "                count += 1\n",
    "                if count >= max_count or (time.time() - start) > timeout:\n",
    "                    return count\n",
    "    return count\n",
    "\n",
    "# Check if already staged\n",
    "if Path(LOCAL_ABC_DIR).exists():\n",
    "    existing = find_abc_files(LOCAL_ABC_DIR, max_count=1000)\n",
    "    if existing >= 1000:\n",
    "        print(f\"‚úÖ Local data already staged: {LOCAL_ABC_DIR} ({existing:,}+ files)\")\n",
    "        RAW_ABC_DIR = LOCAL_ABC_DIR\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Local dir exists but has few files. Re-staging...\")\n",
    "        subprocess.run(['rm', '-rf', LOCAL_ABC_DIR], check=False)\n",
    "        RAW_ABC_DIR = None\n",
    "else:\n",
    "    RAW_ABC_DIR = None\n",
    "\n",
    "if RAW_ABC_DIR is None:\n",
    "    # Extract tarball to local\n",
    "    if not Path(DRIVE_TARBALL).exists():\n",
    "        raise FileNotFoundError(f\"Tarball not found: {DRIVE_TARBALL}\")\n",
    "    \n",
    "    print(f\"üì¶ Extracting {DRIVE_TARBALL} to local VM...\")\n",
    "    print(\"   (This avoids Drive I/O errors on 178K small files)\")\n",
    "    extract_root = Path('/content/abc_extract')\n",
    "    extract_root.mkdir(exist_ok=True)\n",
    "    \n",
    "    subprocess.run(['tar', '-xzf', DRIVE_TARBALL, '-C', str(extract_root)], check=True)\n",
    "    \n",
    "    # Find the .abc directory inside extracted content\n",
    "    for root, dirs, files in os.walk(extract_root):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.abc'):\n",
    "                abc_root = Path(root)\n",
    "                # Move to standard location\n",
    "                subprocess.run(['mv', str(abc_root), LOCAL_ABC_DIR], check=True)\n",
    "                RAW_ABC_DIR = LOCAL_ABC_DIR\n",
    "                break\n",
    "        if RAW_ABC_DIR:\n",
    "            break\n",
    "    \n",
    "    if not RAW_ABC_DIR:\n",
    "        raise RuntimeError(\"Could not find .abc files in tarball\")\n",
    "    \n",
    "    print(f\"‚úÖ Staged to: {RAW_ABC_DIR}\")\n",
    "\n",
    "# Verify\n",
    "count = find_abc_files(RAW_ABC_DIR, max_count=5000)\n",
    "if count == 0:\n",
    "    raise FileNotFoundError(f\"No .abc files found in {RAW_ABC_DIR}\")\n",
    "print(f\"‚úÖ ABC files detected: {count:,}+\")\n",
    "\n",
    "# GPU check\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Change runtime type to GPU (L4 recommended)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup complete. Ready to tokenize from: {RAW_ABC_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 3: INSTALL DEPENDENCIES ===\n",
    "!pip install -q tokenizers scipy matplotlib tqdm\n",
    "print(\"‚úÖ Packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: TOKENIZATION (STREAMING, MEMORY-SAFE) ===\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer_path = f'{PROCESSED_DIR}/music_bpe.json'\n",
    "meta_path = f'{PROCESSED_DIR}/meta.pkl'\n",
    "train_bin = f'{PROCESSED_DIR}/train.bin'\n",
    "val_bin = f'{PROCESSED_DIR}/val.bin'\n",
    "test_bin = f'{PROCESSED_DIR}/test.bin'\n",
    "\n",
    "if os.path.exists(tokenizer_path) and os.path.exists(train_bin):\n",
    "    print(\"‚úÖ Tokenization already complete. Loading metadata...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    print(f\"   Vocab: {meta['vocab_size']}, Train tokens: {meta['train_tokens']:,}\")\n",
    "else:\n",
    "    print(\"üî§ Starting tokenization pipeline...\")\n",
    "    \n",
    "    # Discover all .abc files\n",
    "    abc_files = []\n",
    "    for root, _, files in os.walk(RAW_ABC_DIR):\n",
    "        for fn in files:\n",
    "            if fn.endswith('.abc'):\n",
    "                abc_files.append(os.path.join(root, fn))\n",
    "    \n",
    "    print(f\"   Found {len(abc_files):,} ABC files\")\n",
    "    if len(abc_files) == 0:\n",
    "        raise FileNotFoundError(\"No .abc files discovered\")\n",
    "    \n",
    "    # Deduplicate by hash (streaming)\n",
    "    print(\"   Step 1/4: Deduplicating...\")\n",
    "    seen = set()\n",
    "    unique_files = []\n",
    "    for fp in tqdm(abc_files, desc=\"Dedup\"):\n",
    "        try:\n",
    "            text = Path(fp).read_text(errors='ignore')\n",
    "            if len(text) < 50:\n",
    "                continue\n",
    "            h = hashlib.md5(text.encode('utf-8', errors='ignore')).hexdigest()\n",
    "            if h not in seen:\n",
    "                seen.add(h)\n",
    "                unique_files.append(fp)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"   Kept {len(unique_files):,} unique files\")\n",
    "    \n",
    "    # Train BPE tokenizer\n",
    "    print(\"   Step 2/4: Training BPE tokenizer (vocab=5000)...\")\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    trainer = BpeTrainer(vocab_size=5000, special_tokens=[\"[UNK]\", \"[PAD]\"], show_progress=False)\n",
    "    \n",
    "    def text_iter():\n",
    "        for fp in unique_files:\n",
    "            try:\n",
    "                yield Path(fp).read_text(errors='ignore')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    tokenizer.train_from_iterator(text_iter(), trainer=trainer)\n",
    "    tokenizer.save(tokenizer_path)\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f\"   ‚úÖ Tokenizer saved: vocab={vocab_size}\")\n",
    "    \n",
    "    # Encode to single binary (streaming)\n",
    "    print(\"   Step 3/4: Encoding tokens to disk...\")\n",
    "    all_bin = f'{PROCESSED_DIR}/all.bin'\n",
    "    total_tokens = 0\n",
    "    with open(all_bin, 'wb') as f:\n",
    "        for fp in tqdm(unique_files, desc=\"Encode\"):\n",
    "            try:\n",
    "                text = Path(fp).read_text(errors='ignore')\n",
    "                enc = tokenizer.encode(text)\n",
    "                if enc.ids:\n",
    "                    arr = np.array(enc.ids, dtype=np.uint16)\n",
    "                    arr.tofile(f)\n",
    "                    total_tokens += len(arr)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    \n",
    "    # Split 98/1/1\n",
    "    print(\"   Step 4/4: Splitting train/val/test (98/1/1)...\")\n",
    "    n_train = int(total_tokens * 0.98)\n",
    "    n_val = int(total_tokens * 0.01)\n",
    "    n_test = total_tokens - n_train - n_val\n",
    "    \n",
    "    data = np.memmap(all_bin, dtype=np.uint16, mode='r')\n",
    "    np.asarray(data[:n_train]).tofile(train_bin)\n",
    "    np.asarray(data[n_train:n_train+n_val]).tofile(val_bin)\n",
    "    np.asarray(data[n_train+n_val:]).tofile(test_bin)\n",
    "    del data\n",
    "    os.remove(all_bin)\n",
    "    \n",
    "    meta = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'train_tokens': n_train,\n",
    "        'val_tokens': n_val,\n",
    "        'test_tokens': n_test,\n",
    "        'total_files': len(unique_files)\n",
    "    }\n",
    "    with open(meta_path, 'wb') as f:\n",
    "        pickle.dump(meta, f)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Tokenization complete!\")\n",
    "    print(f\"   Train: {n_train:,} | Val: {n_val:,} | Test: {n_test:,}\")\n",
    "    if n_train >= 100_000_000:\n",
    "        print(\"   ‚úÖ Meets 100M token requirement\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Only {n_train:,} tokens (project asks for 100M+)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data ready in: {PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c434e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: MODEL DEFINITIONS ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    block_size: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.resid_dropout(self.c_proj(y))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(F.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        x = self.embedding(idx)\n",
    "        x, _ = self.lstm(x)\n",
    "        logits = self.fc(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_path, block_size, batch_size):\n",
    "        self.data = np.memmap(data_path, dtype=np.uint16, mode='r')\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
    "        x = torch.stack([torch.from_numpy(self.data[i:i+self.block_size].astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy(self.data[i+1:i+1+self.block_size].astype(np.int64)) for i in ix])\n",
    "        return x, y\n",
    "\n",
    "print(\"‚úÖ Model classes defined (GPT, LSTM, DataLoader)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: TRAIN ALL MODELS (5 GPT + 4 LSTM) ===\n",
    "import time\n",
    "import json\n",
    "\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "VOCAB_SIZE = meta['vocab_size']\n",
    "BLOCK_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = meta['train_tokens'] // (BATCH_SIZE * BLOCK_SIZE)\n",
    "\n",
    "print(f\"üìä Training config: vocab={VOCAB_SIZE}, block={BLOCK_SIZE}, batch={BATCH_SIZE}\")\n",
    "print(f\"   Steps per epoch (1 pass over data): {STEPS_PER_EPOCH:,}\")\n",
    "\n",
    "def get_lr(step, max_steps, base_lr):\n",
    "    warmup = 500\n",
    "    if step < warmup:\n",
    "        return base_lr * (step + 1) / warmup\n",
    "    progress = (step - warmup) / (max_steps - warmup)\n",
    "    return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "def train_one_epoch(model, config, name):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    decay = [p for n, p in model.named_parameters() if p.requires_grad and p.dim() >= 2]\n",
    "    nodecay = [p for n, p in model.named_parameters() if p.requires_grad and p.dim() < 2]\n",
    "    optim = torch.optim.AdamW([\n",
    "        {'params': decay, 'weight_decay': config['wd']},\n",
    "        {'params': nodecay, 'weight_decay': 0.0}\n",
    "    ], lr=config['lr'])\n",
    "    \n",
    "    train_loader = DataLoader(train_bin, BLOCK_SIZE, BATCH_SIZE)\n",
    "    val_loader = DataLoader(val_bin, BLOCK_SIZE, BATCH_SIZE)\n",
    "    \n",
    "    # Resume from checkpoint\n",
    "    ckpt_path = f\"{CHECKPOINT_DIR}/{name}_checkpoint.pt\"\n",
    "    start_step = 0\n",
    "    val_losses = []\n",
    "    if os.path.exists(ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(ckpt['model'])\n",
    "        optim.load_state_dict(ckpt['optimizer'])\n",
    "        start_step = ckpt['step']\n",
    "        val_losses = ckpt.get('val_losses', [])\n",
    "        print(f\"   ‚Ü©Ô∏è Resuming from step {start_step}\")\n",
    "    \n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    pbar = tqdm(range(start_step, STEPS_PER_EPOCH), desc=name)\n",
    "    for step in pbar:\n",
    "        lr = get_lr(step, STEPS_PER_EPOCH, config['lr'])\n",
    "        for g in optim.param_groups:\n",
    "            g['lr'] = lr\n",
    "        \n",
    "        x, y = train_loader.get_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        _, loss = model(x, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'lr': f\"{lr:.2e}\"})\n",
    "        \n",
    "        # Checkpoint every 1000 steps\n",
    "        if (step + 1) % 1000 == 0 or step == STEPS_PER_EPOCH - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                xv, yv = val_loader.get_batch()\n",
    "                _, vl = model(xv.to(device), yv.to(device))\n",
    "                val_losses.append(vl.item())\n",
    "            model.train()\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optim.state_dict(),\n",
    "                'step': step + 1,\n",
    "                'val_losses': val_losses\n",
    "            }, ckpt_path)\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    final_val = val_losses[-1] if val_losses else loss.item()\n",
    "    \n",
    "    # Save final\n",
    "    final_path = f\"{CHECKPOINT_DIR}/{name}_final.pt\"\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'val_loss': final_val,\n",
    "        'time': elapsed,\n",
    "        'config': config\n",
    "    }, final_path)\n",
    "    \n",
    "    print(f\"   ‚úÖ Done: val_loss={final_val:.4f}, time={elapsed/3600:.2f}h\")\n",
    "    return final_val, elapsed\n",
    "\n",
    "# 9 experiments\n",
    "experiments = [\n",
    "    {'name': 'gpt_tiny',   'type': 'gpt', 'n_layer': 2,  'n_embd': 128, 'n_head': 4,  'lr': 1e-3, 'wd': 0.01},\n",
    "    {'name': 'gpt_small',  'type': 'gpt', 'n_layer': 4,  'n_embd': 256, 'n_head': 4,  'lr': 6e-4, 'wd': 0.01},\n",
    "    {'name': 'gpt_medium', 'type': 'gpt', 'n_layer': 6,  'n_embd': 384, 'n_head': 6,  'lr': 3e-4, 'wd': 0.1},\n",
    "    {'name': 'gpt_large',  'type': 'gpt', 'n_layer': 10, 'n_embd': 512, 'n_head': 8,  'lr': 2e-4, 'wd': 0.1},\n",
    "    {'name': 'gpt_xl',     'type': 'gpt', 'n_layer': 16, 'n_embd': 768, 'n_head': 12, 'lr': 1e-4, 'wd': 0.1},\n",
    "    {'name': 'lstm_small', 'type': 'lstm', 'layers': 2, 'hidden': 256,  'lr': 1e-3, 'wd': 0.0},\n",
    "    {'name': 'lstm_medium','type': 'lstm', 'layers': 2, 'hidden': 512,  'lr': 6e-4, 'wd': 0.0},\n",
    "    {'name': 'lstm_large', 'type': 'lstm', 'layers': 3, 'hidden': 768,  'lr': 3e-4, 'wd': 0.0},\n",
    "    {'name': 'lstm_xl',    'type': 'lstm', 'layers': 4, 'hidden': 1024, 'lr': 1e-4, 'wd': 0.0},\n",
    "]\n",
    "\n",
    "results_pkl = f'{OUTPUT_DIR}/all_results.pkl'\n",
    "if os.path.exists(results_pkl):\n",
    "    with open(results_pkl, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "else:\n",
    "    all_results = {}\n",
    "\n",
    "print(f\"\\nüöÄ Training {len(experiments)} models (skips already done)\\n\")\n",
    "\n",
    "for exp in experiments:\n",
    "    name = exp['name']\n",
    "    final_path = f\"{CHECKPOINT_DIR}/{name}_final.pt\"\n",
    "    \n",
    "    if os.path.exists(final_path) and name in all_results:\n",
    "        print(f\"‚è© Skipping {name} (already trained)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n‚ñ∂Ô∏è  Training {name}\")\n",
    "    \n",
    "    try:\n",
    "        if exp['type'] == 'gpt':\n",
    "            model = GPTModel(GPTConfig(\n",
    "                vocab_size=VOCAB_SIZE,\n",
    "                n_layer=exp['n_layer'],\n",
    "                n_embd=exp['n_embd'],\n",
    "                n_head=exp['n_head'],\n",
    "                block_size=BLOCK_SIZE\n",
    "            ))\n",
    "        else:\n",
    "            model = LSTMModel(VOCAB_SIZE, exp['hidden'], exp['layers'])\n",
    "        \n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"   Params: {n_params:,}\")\n",
    "        \n",
    "        val_loss, train_time = train_one_epoch(model, {'lr': exp['lr'], 'wd': exp['wd']}, name)\n",
    "        \n",
    "        all_results[name] = {\n",
    "            'type': exp['type'],\n",
    "            'params': n_params,\n",
    "            'val_loss': val_loss,\n",
    "            'time_h': train_time / 3600,\n",
    "            'config': exp\n",
    "        }\n",
    "        \n",
    "        with open(results_pkl, 'wb') as f:\n",
    "            pickle.dump(all_results, f)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save JSON summary\n",
    "summary = {k: {'params': v['params'], 'val_loss': v['val_loss'], 'time_h': v['time_h']} for k, v in all_results.items()}\n",
    "with open(f'{OUTPUT_DIR}/scaling_results.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! {len(all_results)} models trained.\")\n",
    "print(f\"   Results: {OUTPUT_DIR}/scaling_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0498d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: SCALING LAW + GENERATION + MIDI ===\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "print(\"üìà Step 1/3: Fitting scaling laws...\\n\")\n",
    "\n",
    "# Separate GPT vs LSTM\n",
    "gpt_res = {k: v for k, v in all_results.items() if v['type'] == 'gpt'}\n",
    "lstm_res = {k: v for k, v in all_results.items() if v['type'] == 'lstm'}\n",
    "\n",
    "gpt_params = np.array([r['params'] for r in gpt_res.values()])\n",
    "gpt_losses = np.array([r['val_loss'] for r in gpt_res.values()])\n",
    "lstm_params = np.array([r['params'] for r in lstm_res.values()])\n",
    "lstm_losses = np.array([r['val_loss'] for r in lstm_res.values()])\n",
    "\n",
    "def power_law(N, a, alpha, c):\n",
    "    return a * N**(-alpha) + c\n",
    "\n",
    "# Fit GPT\n",
    "try:\n",
    "    gpt_fit, _ = curve_fit(power_law, gpt_params, gpt_losses, p0=[1.0, 0.1, 1.0], maxfev=10000)\n",
    "    ga, galpha, gc = gpt_fit\n",
    "    print(f\"‚úÖ GPT: L = {ga:.4f}¬∑N^(-{galpha:.4f}) + {gc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå GPT fit failed: {e}\")\n",
    "    ga, galpha, gc = None, None, None\n",
    "\n",
    "# Fit LSTM\n",
    "try:\n",
    "    lstm_fit, _ = curve_fit(power_law, lstm_params, lstm_losses, p0=[1.0, 0.1, 1.0], maxfev=10000)\n",
    "    la, lalpha, lc = lstm_fit\n",
    "    print(f\"‚úÖ LSTM: L = {la:.4f}¬∑N^(-{lalpha:.4f}) + {lc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LSTM fit failed: {e}\")\n",
    "    la, lalpha, lc = None, None, None\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.scatter(gpt_params, gpt_losses, s=100, c='blue', alpha=0.7, label='GPT')\n",
    "for i, name in enumerate(gpt_res.keys()):\n",
    "    ax1.annotate(name, (gpt_params[i], gpt_losses[i]), xytext=(5,5), textcoords='offset points', fontsize=8)\n",
    "if ga:\n",
    "    N_fit = np.logspace(np.log10(gpt_params.min()), np.log10(gpt_params.max()), 100)\n",
    "    ax1.plot(N_fit, power_law(N_fit, ga, galpha, gc), 'r--', linewidth=2, label=f'Œ±={galpha:.4f}')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('Parameters')\n",
    "ax1.set_ylabel('Val Loss')\n",
    "ax1.set_title('GPT Scaling Law')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.scatter(lstm_params, lstm_losses, s=100, c='green', alpha=0.7, label='LSTM')\n",
    "for i, name in enumerate(lstm_res.keys()):\n",
    "    ax2.annotate(name, (lstm_params[i], lstm_losses[i]), xytext=(5,5), textcoords='offset points', fontsize=8)\n",
    "if la:\n",
    "    N_fit = np.logspace(np.log10(lstm_params.min()), np.log10(lstm_params.max()), 100)\n",
    "    ax2.plot(N_fit, power_law(N_fit, la, lalpha, lc), 'r--', linewidth=2, label=f'Œ±={lalpha:.4f}')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Parameters')\n",
    "ax2.set_ylabel('Val Loss')\n",
    "ax2.set_title('LSTM Scaling Law')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/scaling_laws.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"‚úÖ Plot saved: {OUTPUT_DIR}/scaling_laws.png\\n\")\n",
    "\n",
    "# Save params\n",
    "with open(f'{OUTPUT_DIR}/scaling_params.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'gpt': {'a': float(ga) if ga else None, 'alpha': float(galpha) if galpha else None, 'c': float(gc) if gc else None},\n",
    "        'lstm': {'a': float(la) if la else None, 'alpha': float(lalpha) if lalpha else None, 'c': float(lc) if lc else None}\n",
    "    }, f, indent=2)\n",
    "\n",
    "# === GENERATION ===\n",
    "print(\"üéµ Step 2/3: Generating music samples...\\n\")\n",
    "\n",
    "best_name = min(all_results.items(), key=lambda x: x[1]['val_loss'])[0]\n",
    "best = all_results[best_name]\n",
    "print(f\"Using best model: {best_name} (val_loss={best['val_loss']:.4f})\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt = torch.load(f\"{CHECKPOINT_DIR}/{best_name}_final.pt\", map_location=device)\n",
    "\n",
    "cfg = best['config']\n",
    "if best['type'] == 'gpt':\n",
    "    model = GPTModel(GPTConfig(vocab_size=VOCAB_SIZE, n_layer=cfg['n_layer'], n_embd=cfg['n_embd'], n_head=cfg['n_head'], block_size=BLOCK_SIZE)).to(device)\n",
    "else:\n",
    "    model = LSTMModel(VOCAB_SIZE, cfg['hidden'], cfg['layers']).to(device)\n",
    "\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "def nucleus_sample(logits, top_p=0.9, temp=0.8):\n",
    "    logits = logits / temp\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "    cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = cum > top_p\n",
    "    mask[..., 1:] = mask[..., :-1].clone()\n",
    "    mask[..., 0] = 0\n",
    "    sorted_probs[mask] = 0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "    return sorted_idx[torch.multinomial(sorted_probs, 1)]\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(prompt, max_tokens=400):\n",
    "    enc = tokenizer.encode(prompt)\n",
    "    tokens = torch.tensor(enc.ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    for _ in range(max_tokens):\n",
    "        crop = tokens[:, -BLOCK_SIZE:]\n",
    "        logits, _ = model(crop)\n",
    "        next_tok = nucleus_sample(logits[:, -1, :])\n",
    "        tokens = torch.cat([tokens, next_tok], dim=1)\n",
    "    return tokenizer.decode(tokens[0].tolist())\n",
    "\n",
    "prompts = [\n",
    "    \"X:1\\nM:4/4\\nK:C\\n\", \"X:1\\nM:6/8\\nK:G\\n\", \"X:1\\nM:3/4\\nK:D\\n\",\n",
    "    \"X:1\\nM:4/4\\nK:Am\\n\", \"X:1\\nM:2/4\\nK:F\\n\", \"X:1\\nM:4/4\\nK:Em\\n\",\n",
    "    \"X:1\\nM:6/8\\nK:A\\n\", \"X:1\\nM:3/4\\nK:Bm\\n\", \"X:1\\nM:4/4\\nK:E\\n\",\n",
    "    \"X:1\\nM:2/2\\nK:Bb\\n\", \"X:1\\nM:9/8\\nK:D\\n\", \"X:1\\nM:5/4\\nK:Gm\\n\"\n",
    "]\n",
    "\n",
    "samples_dir = f'{OUTPUT_DIR}/generated_samples'\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "samples = []\n",
    "\n",
    "for i, prompt in enumerate(tqdm(prompts, desc=\"Generate\")):\n",
    "    text = generate(prompt)\n",
    "    path = f'{samples_dir}/sample_{i+1:02d}.abc'\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(text)\n",
    "    samples.append({'id': i+1, 'prompt': prompt, 'text': text, 'path': path})\n",
    "\n",
    "print(f\"‚úÖ Generated {len(samples)} samples\\n\")\n",
    "\n",
    "# === MIDI CONVERSION ===\n",
    "print(\"üéπ Step 3/3: Converting to MIDI & computing metrics...\\n\")\n",
    "\n",
    "try:\n",
    "    subprocess.run(['abc2midi', '-h'], capture_output=True)\n",
    "except:\n",
    "    os.system('apt-get update -qq && apt-get install -y abcmidi')\n",
    "\n",
    "def is_valid_abc(text):\n",
    "    return all(h in text for h in ['X:', 'M:', 'K:'])\n",
    "\n",
    "def to_midi(abc_path, midi_path):\n",
    "    try:\n",
    "        r = subprocess.run(['abc2midi', abc_path, '-o', midi_path], capture_output=True, timeout=20)\n",
    "        return r.returncode == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "midi_dir = f'{OUTPUT_DIR}/generated_midi'\n",
    "os.makedirs(midi_dir, exist_ok=True)\n",
    "\n",
    "valid = 0\n",
    "converted = 0\n",
    "for s in samples:\n",
    "    if is_valid_abc(s['text']):\n",
    "        valid += 1\n",
    "    midi_path = f\"{midi_dir}/sample_{s['id']:02d}.mid\"\n",
    "    if to_midi(s['path'], midi_path):\n",
    "        converted += 1\n",
    "        s['midi'] = midi_path\n",
    "\n",
    "pct_valid = 100 * valid / len(samples)\n",
    "pct_midi = 100 * converted / len(samples)\n",
    "perplexity = float(np.exp(best['val_loss']))\n",
    "\n",
    "metrics = {\n",
    "    'model': best_name,\n",
    "    'params': best['params'],\n",
    "    'val_loss': best['val_loss'],\n",
    "    'perplexity': perplexity,\n",
    "    'samples': len(samples),\n",
    "    'valid_pct': pct_valid,\n",
    "    'midi_pct': pct_midi\n",
    "}\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Perplexity: {perplexity:.2f}\")\n",
    "print(f\"‚úÖ Valid ABC: {valid}/{len(samples)} ({pct_valid:.1f}%)\")\n",
    "print(f\"‚úÖ MIDI converted: {converted}/{len(samples)} ({pct_midi:.1f}%)\")\n",
    "print(f\"\\n‚úÖ All outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3e020",
   "metadata": {},
   "source": [
    "## üéâ COMPLETE!\n",
    "\n",
    "### Files Created\n",
    "\n",
    "**In Drive (`/MyDrive/NYU_ML_Project/`):**\n",
    "\n",
    "üìÅ `Data/processed_v3/`\n",
    "- `music_bpe.json` ‚Äî BPE tokenizer (vocab=5000)\n",
    "- `train.bin`, `val.bin`, `test.bin` ‚Äî 98/1/1 split\n",
    "- `meta.pkl` ‚Äî token counts\n",
    "\n",
    "üìÅ `checkpoints_v3/`\n",
    "- `gpt_tiny_final.pt` ... `lstm_xl_final.pt` ‚Äî 9 trained models\n",
    "- `*_checkpoint.pt` ‚Äî resume points\n",
    "\n",
    "üìÅ `outputs/`\n",
    "- `scaling_results.json` ‚Äî all training results\n",
    "- `scaling_params.json` ‚Äî fitted Œ± exponents\n",
    "- `scaling_laws.png` ‚Äî log-log plots\n",
    "- `metrics.json` ‚Äî perplexity, % valid, % MIDI\n",
    "- `generated_samples/*.abc` ‚Äî 12 generated songs\n",
    "- `generated_midi/*.mid` ‚Äî MIDI files\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Download results:** Right-click `outputs/` ‚Üí Download\n",
    "2. **Listen to music:** Open `.mid` files in any MIDI player\n",
    "3. **Write report (6-10 pages):**\n",
    "   - Intro + motivation\n",
    "   - Dataset (178K ABC files, 100M+ tokens)\n",
    "   - Methods (BPE tokenization, 9 models, 1 epoch)\n",
    "   - Results (scaling laws, Œ± exponents, plots)\n",
    "   - Analysis (sample quality, MIDI conversion rates)\n",
    "   - Conclusion\n",
    "4. **Submit by Dec 15, 2025**\n",
    "\n",
    "---\n",
    "\n",
    "### Key Results\n",
    "\n",
    "Check these files for your report:\n",
    "- **GPT Œ±:** `outputs/scaling_params.json` ‚Üí `gpt.alpha`\n",
    "- **LSTM Œ±:** `outputs/scaling_params.json` ‚Üí `lstm.alpha`\n",
    "- **Best model:** `outputs/metrics.json` ‚Üí `model`\n",
    "- **Perplexity:** `outputs/metrics.json` ‚Üí `perplexity`\n",
    "- **Sample quality:** `outputs/metrics.json` ‚Üí `valid_pct`, `midi_pct`\n",
    "\n",
    "---\n",
    "\n",
    "**Total runtime:** ~20-24 hours (mostly Cell 6 training)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
